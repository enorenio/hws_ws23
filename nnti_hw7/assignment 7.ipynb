{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "NNTI Assignment 7\n",
    "\n",
    "December 21, 2023\n",
    "\n",
    "Name: Aleksey Morshnev\n",
    "Student ID: 7042691\n",
    "Email: almo00008@stud.uni-saarland.de\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7.3 (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will build the toy library. You are asked to perform backpropagation on a neural network model that you will build in this exercise.  \n",
    "  \n",
    "In this toy library, we are not implementing the functionalities of autograd or any other automatic differentiation. Still it will be extremely helpful for you to know the basics about how the PyTorch autograd functionality works (e.g. for checking your implementation of gradient calculations). A good starting point would be [PyTorch autograd tutorial](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html).   \n",
    "\n",
    "All classes that you will implement must have a `grad` function which would compute and return gradients. The `grad` function in the classes of the following *loss* funtions (`MSELoss` and `CrossEntropyLoss`) must compute gradients of the loss w.r.t. its input. The `grad` function for *activation functions* must take the incoming gradient (possibly from the previous layer or the loss function) and compute gradients of the loss w.r.t. its input. The `grad` function for *layers* (in this exercise we have only `Linear` layer) must take the incoming gradient and compute gradients of the loss w.r.t. both its input and weights (you can ignore computing the gradients w.r.t. biases).\n",
    "\n",
    "For each gradient calculation, we are providing some low-dimensional data. After you finish the implementation of each `grad` function, simply run the corresponding cell (**do not** change the contents of these cells). To check for the correctness of the implementation, we ask you to *call* the corresponding function from PyTorch on the same input data, compute gradients, and compare them with the gradients from your implementation. If you have a correct solution, then they must be the same (or maybe with some very small <$10^{-3}$ differences).\n",
    "\n",
    "Please remember that everything is processed in minibatches and gradients must be calculated accordingly. The input for each of the model components has dimensions of `N*D` where `N` is the number of datapoints in minibatch and `D` is the number of features. Of course, all the gradient computations must, ideally, be implemented in vectorized form (without using any loops)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: do not modify the code in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from activations import ReLU, Sigmoid\n",
    "from losses import CrossEntropy, MSELoss\n",
    "from layers import Linear, Dropout\n",
    "from model import Model\n",
    "\n",
    "import torch\n",
    "np.random.seed(23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7.3.1 Implement __ call __ and grad methods for MSE loss (0.25 points)\n",
    "Check the correctness of the gradient by calculating it on the same data using PyTorch  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array([0, 1, 2])\n",
    "y_true = np.array([1, 3, 3])\n",
    "loss = MSELoss()\n",
    "\n",
    "print('predictions')\n",
    "print(y_pred)\n",
    "print('true values')\n",
    "print(y_true)\n",
    "\n",
    "print('MSE loss')\n",
    "print(loss(y_pred, y_true))\n",
    "\n",
    "print('MSE gradient')\n",
    "print(loss.grad())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = torch.tensor([0, 1, 2], requires_grad=True, dtype=torch.float32)\n",
    "y_true = torch.tensor([1, 3, 3], dtype=torch.float32)\n",
    "loss = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "print('predictions')\n",
    "print(y_pred)\n",
    "print('true values')\n",
    "print(y_true)\n",
    "\n",
    "print('MSE loss')\n",
    "out = loss(y_pred, y_true)\n",
    "print(out.item())\n",
    "out.backward()\n",
    "\n",
    "print('MSE gradient')\n",
    "print(y_pred.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7.3.2 Implement __ call __ and grad methods for Cross Entropy Loss (0.75 points)\n",
    "<br> $\\frac{\\delta L}{\\delta o_i} = p_i - y_i$  \n",
    "where <br> $o_i$ - one of the input variables,   \n",
    "$p_i$ - probability for that input variable calculated using softmax,   \n",
    "$y_i$ - label for that input variable ($y_i \\in \\{0, 1\\}$).  \n",
    "For simplicity of the proof, you can prove it for just one datapoint, but in the code, you should properly extrapolate it for computing the gradients for the whole minibatch (`N` datapoints).  \n",
    "  \n",
    "Please remember that a typical Cross Entropy Loss implementation, including ours, implicitly applies Softmax before calculating the CE loss.\n",
    "  \n",
    "Check the correctness of the gradient by calculating it on the same data using PyTorch  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_loss = CrossEntropy(average=True)\n",
    "predictions = np.array([[0.4,0.35,0.71,0.30],\n",
    "                        [0.01,0.01,0.01,0.65]])\n",
    "targets = np.array([[0,0,1,0],\n",
    "                  [0,0,0,1]])\n",
    "\n",
    "print('predictions')\n",
    "print(predictions)\n",
    "print('targets')\n",
    "print(targets)\n",
    "\n",
    "print('cross entropy loss')\n",
    "print(ce_loss(predictions, targets))\n",
    "\n",
    "print('gradient of the cross entropy loss')\n",
    "print(ce_loss.grad())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_loss = torch.nn.CrossEntropyLoss()\n",
    "predictions = torch.tensor([[0.4,0.35,0.71,0.30],\n",
    "                            [0.01,0.01,0.01,0.65]], dtype=torch.float32, requires_grad=True)\n",
    "targets = torch.tensor([[0,0,1,0],\n",
    "                        [0,0,0,1]], dtype=torch.float32)\n",
    "\n",
    "print('predictions')\n",
    "print(predictions)\n",
    "print('targets')\n",
    "print(targets)\n",
    "\n",
    "out = ce_loss(predictions, targets)\n",
    "print(out.item())\n",
    "\n",
    "out.backward()\n",
    "print(predictions.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7.3.3 Implement the __ call __ and grad methods for linear layer (1.0 points)\n",
    "\n",
    "$\\frac{\\delta L}{\\delta X} = \\frac{\\delta L}{\\delta Y} W^T$ and $\\frac{\\delta L}{\\delta W} = X^T \\frac{\\delta L}{\\delta Y}$  \n",
    "where $Y = XW$ <br> (X - input data matrix of dimension `N * in_features` and W is a weight matrix of dimension `in_features * out_features`),  \n",
    "$\\frac{\\delta L}{\\delta Y}$ is the incoming gradient of dimension `N * out_features` (e.g. from the loss function that is applied on the outputs of the linear layer).  \n",
    "\n",
    "Check the correctness of the gradient by calculating it on the same data using PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, to get the same output, the *weights* and *biases* of the `Linear` layer instantiated above and the `Linear` layer from PyTorch must be the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch_size = 4\n",
    "in_features = 5\n",
    "out_features = 2\n",
    "minibatch = np.random.randn(minibatch_size, in_features)\n",
    "print('input data')\n",
    "print(minibatch)\n",
    "\n",
    "layer = Linear(in_features, out_features)\n",
    "print('output of the linear layer')\n",
    "print(layer(minibatch))\n",
    "\n",
    "in_gradient = np.ones((minibatch_size, out_features,))\n",
    "gradient_weights, gradient_input = layer.grad(in_gradient)\n",
    "print('gradient w.r.t weights')\n",
    "print(gradient_weights)\n",
    "print('gradient w.r.t. inputs')\n",
    "print(gradient_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('input data')\n",
    "minibatch = torch.tensor(minibatch, dtype=torch.float32, requires_grad=True)\n",
    "print(minibatch)\n",
    "\n",
    "linear = torch.nn.Linear(in_features, out_features, bias=True)\n",
    "with torch.no_grad():\n",
    "    linear.weight.copy_(torch.tensor(layer.weights).t())\n",
    "    linear.bias.copy_(torch.tensor(layer.bias[0,:]))\n",
    "out = linear(minibatch)\n",
    "print('output of the linear layer')\n",
    "print(out)\n",
    "\n",
    "in_gradient = torch.ones((minibatch_size, out_features,))\n",
    "out.backward(gradient=in_gradient)\n",
    "print('gradient w.r.t weights')\n",
    "print(linear.weight.grad.t())\n",
    "print('gradient w.r.t. inputs')\n",
    "print(minibatch.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7.3.4 Implement __ call __ and grad methods for activation functions (0.5 point)\n",
    "Check the correctness of the gradients by calculating them on the same data using PyTorch.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0.1, -0.3, 0.5, 0.9, 0, -1.0],\n",
    "              [0.2, -0.4, 1.1, 0.4, 0.3, 0]])\n",
    "sigmoid = Sigmoid()\n",
    "print(sigmoid(x))\n",
    "\n",
    "in_gradient = np.ones((2, 6,))\n",
    "print(sigmoid.grad(in_gradient))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_torch = torch.tensor(x, requires_grad=True)\n",
    "torch_sigmoid = torch.nn.Sigmoid()\n",
    "out = torch_sigmoid(x_torch)\n",
    "print(out)\n",
    "\n",
    "in_gradient_torch = torch.ones((2, 6, ))\n",
    "out.backward(gradient=in_gradient_torch)\n",
    "print(x_torch.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu = ReLU()\n",
    "print(relu(x))\n",
    "print(relu.grad(in_gradient))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_torch = torch.tensor(x, requires_grad=True)\n",
    "torch_relu = torch.nn.ReLU()\n",
    "out = torch_relu(x_torch)\n",
    "print(out)\n",
    "\n",
    "in_gradient_torch = torch.ones((2, 6, ))\n",
    "out.backward(gradient=in_gradient_torch)\n",
    "print(x_torch.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7.3.5 Implement a model class (2.0 points)\n",
    "Implement a model class which stores a list of components of the model (in this exercise those are only the *layers* and *activation functions*). \n",
    "It must perform the forward pass and also be able to calculate and store the gradients for all the layers, and perform a parameter update step (here we deviate from PyTorch since we don't use *autograd*).  \n",
    "For simplicity, you don't have to compare the value of each parameter of the model with PyTorch implementation, but just check the value of the resultant loss (before and after the parameter update step). We provide all the code, including the code for PyTorch below. You don't have to change the cells below, but just check whether your implementation of the model achieves the same decrease in loss as the equivalent implementation in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Model\n",
    "np.random.seed(123)\n",
    "\n",
    "layer1 = Linear(1000, 100)\n",
    "activation1 = ReLU()\n",
    "layer2 = Linear(100, 10)\n",
    "activation2 = ReLU()\n",
    "loss = CrossEntropy()\n",
    "\n",
    "x = np.random.randn(2, 1000)\n",
    "y_true = np.zeros((2, 10,))\n",
    "y_true[0, 4] = 1\n",
    "y_true[1, 1] = 1\n",
    "m = Model([layer1, activation1, layer2, activation2])\n",
    "out = m.forward(x)\n",
    "print(loss(out, y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(1000, 100, bias=True)\n",
    "        self.layer2 = nn.Linear(100, 10, bias=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return x\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "net = Net()\n",
    "\n",
    "with torch.no_grad():\n",
    "    net.layer1.weight.copy_(torch.tensor(layer1.weights).t())\n",
    "    net.layer1.bias.copy_(torch.tensor(layer1.bias[0,:]))\n",
    "    net.layer2.weight.copy_(torch.tensor(layer2.weights).t())\n",
    "    net.layer2.bias.copy_(torch.tensor(layer2.bias[0,:]))\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0)\n",
    "\n",
    "x_torch = torch.tensor(x, dtype=torch.float32)\n",
    "out = net(x_torch)\n",
    "y_true_torch = torch.tensor(y_true, dtype=torch.float32)\n",
    "loss_torch = criterion(out, y_true_torch)\n",
    "print(loss_torch.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = m.backward(loss.grad())\n",
    "m.update_parameters(grads, 0.001)\n",
    "out = m.forward(x)\n",
    "model_loss_ours = loss(out, y_true)\n",
    "print(model_loss_ours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_torch.backward()\n",
    "optimizer.step()\n",
    "\n",
    "out = net(x_torch)\n",
    "model_loss_pt = criterion(out, y_true_torch).item()\n",
    "print(model_loss_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check within some acceptable tolerance level\n",
    "\n",
    "np.allclose(model_loss_ours, model_loss_pt, atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7.3.6 Implement __ call __ and grad methods for dropout (0.5 point)\n",
    "In this exercise we are going to implement inverted dropout. \n",
    "We implement dropout as a layer wrapper where Dropout class takes two arguments\n",
    "Dropout (layer, probability). Although dropout can be applied to several types\n",
    "of layers, we only apply it to linear layers in this exercise. Use inverted dropout\n",
    "in this exercise. Implement dropout in ./layers/Dropout.py which transforms the input by setting randomly chosen activations to 0 by a probability p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "layer1 = Linear(1000, 100)\n",
    "activation1 = ReLU()\n",
    "layer2 = Dropout(Linear(100, 10), p=0.5)\n",
    "activation2 = ReLU()\n",
    "loss = CrossEntropy()\n",
    "\n",
    "x = np.random.randn(2, 1000)\n",
    "y_true = np.zeros((2, 10,))\n",
    "y_true[0, 4] = 1\n",
    "y_true[1, 1] = 1\n",
    "m = Model([layer1, activation1, layer2, activation2])\n",
    "out = m.forward(x)\n",
    "\n",
    "# numpy seed is fixed so you should get the same value after each run\n",
    "# print(loss(out, y_true)) # = 2.15028 with tolerance 5e-3\n",
    "np.allclose(loss(out, y_true), 2.15028, atol=5e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
