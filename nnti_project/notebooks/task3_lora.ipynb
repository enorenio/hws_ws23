{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "import torch\n",
    "from functools import partial\n",
    "import time\n",
    "\n",
    "DATASET = 'hackathon-pln-es/spanish-to-quechua'\n",
    "MODEL_NAME = 'facebook/xglm-564M'\n",
    "SEQ_LEN   = 32"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T04:46:28.327140Z",
     "start_time": "2024-03-17T04:46:24.377143Z"
    }
   },
   "id": "475d76d5ef831198",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def getDataset():\n",
    "\n",
    "    print(f'\\nin getDataset')\n",
    "\n",
    "    #data and tokenizer\n",
    "    data = load_dataset(DATASET)\n",
    "    tokenizer = getTokenizer(MODEL_NAME)\n",
    "\n",
    "    print(data)\n",
    "\n",
    "    #split data\n",
    "    # data = data[\"train\"].train_test_split(test_size=.2, seed=1)\n",
    "\n",
    "    data = data.map( preprocess,\n",
    "                     # batched = True,\n",
    "                     # num_proc = 4,\n",
    "                     fn_kwargs = {'tokenizer' : tokenizer},\n",
    "                     remove_columns = data['train'].column_names\n",
    "                     )\n",
    "\n",
    "    lm_dataset = data.map(group_texts,\n",
    "                          batched=True,\n",
    "                          num_proc=4,\n",
    "                          fn_kwargs = {'block_size' : SEQ_LEN } )\n",
    "\n",
    "    print(lm_dataset['train'])\n",
    "    print(lm_dataset['train'][0])\n",
    "\n",
    "    return lm_dataset\n",
    "\n",
    "def getTokenizer(TOKENIZER):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER)\n",
    "    # tokenizer.pad_token = tokenizer.eos_token\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def preprocess(data_row, tokenizer):\n",
    "    return tokenizer(data_row['qu'])\n",
    "\n",
    "def group_texts(examples, block_size):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "\n",
    "    # if total_length >= block_size:\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "\n",
    "    # Split by chunks of block_size.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "\n",
    "    # labels because the model expects the argument to be named labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    # del result['input_ids']\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T04:46:51.113710Z",
     "start_time": "2024-03-17T04:46:51.108220Z"
    }
   },
   "id": "d487f8cf5806993d",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-17T04:46:53.643846Z",
     "start_time": "2024-03-17T04:46:53.634433Z"
    }
   },
   "outputs": [],
   "source": [
    "class LoRALayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        super().__init__()\n",
    "        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())\n",
    "        self.A = torch.nn.Parameter(torch.randn(in_dim, rank) * std_dev)\n",
    "        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.alpha * (x @ self.A @ self.B)\n",
    "        return x\n",
    "\n",
    "class LinearWithLoRA(torch.nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora(x)\n",
    "\n",
    "\n",
    "def get_lora_model(model):\n",
    "    # default hyperparameter choices\n",
    "    lora_r = 8\n",
    "    lora_alpha = 16\n",
    "    lora_dropout = 0.05\n",
    "    lora_query = True\n",
    "    lora_key = True\n",
    "    lora_value = True\n",
    "    lora_projection = True\n",
    "    lora_mlp = True\n",
    "    lora_head = False\n",
    "\n",
    "    assign_lora = partial(LinearWithLoRA, rank=lora_r, alpha=lora_alpha)\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for layer in model.model.layers:\n",
    "        if lora_query:\n",
    "            layer.self_attn.q_proj = assign_lora(layer.self_attn.q_proj)\n",
    "        if lora_key:\n",
    "            layer.self_attn.k_proj = assign_lora(layer.self_attn.k_proj)\n",
    "        if lora_value:\n",
    "            layer.self_attn.v_proj = assign_lora(layer.self_attn.v_proj)\n",
    "        if lora_projection:\n",
    "            layer.self_attn.out_proj = assign_lora(layer.self_attn.out_proj)\n",
    "        if lora_mlp:\n",
    "            layer.fc1 = assign_lora(layer.fc1)\n",
    "            layer.fc2 = assign_lora(layer.fc2)\n",
    "\n",
    "    if lora_head:\n",
    "        model.model.lm_head = assign_lora(model.model.lm_head)\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_XGLM_lora():\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "    print(model)\n",
    "    model_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total trainable parameters in model : {model_params}\")\n",
    "\n",
    "    lora_model = get_lora_model(model)\n",
    "    print(lora_model)\n",
    "    lora_model_params = sum(p.numel() for p in lora_model.parameters() if p.requires_grad)\n",
    "    print(f\"Total trainable parameters in lora model : {lora_model_params} and are {(lora_model_params/model_params)*100} % of the original model\")\n",
    "\n",
    "    lm_dataset = getDataset()\n",
    "    train_XGLM(lora_model, lm_dataset, \"xglm_lora\")\n",
    "\n",
    "\n",
    "def train_XGLM(model, lm_dataset, output_dir):\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "        #push_to_hub=True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=lm_dataset[\"train\"],\n",
    "        eval_dataset=lm_dataset[\"validation\"],\n",
    "        # data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    st = time.time()\n",
    "    trainer.train()\n",
    "    et = time.time()\n",
    "\n",
    "    print(f\"total training time : {(et - st)} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/reni/miniconda3/envs/hws_ws23/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGLMForCausalLM(\n",
      "  (model): XGLMModel(\n",
      "    (embed_tokens): Embedding(256008, 1024, padding_idx=1)\n",
      "    (embed_positions): XGLMSinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x XGLMDecoderLayer(\n",
      "        (self_attn): XGLMAttention(\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (activation_fn): GELUActivation()\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=256008, bias=False)\n",
      ")\n",
      "Total trainable parameters in model : 564463616\n",
      "XGLMForCausalLM(\n",
      "  (model): XGLMModel(\n",
      "    (embed_tokens): Embedding(256008, 1024, padding_idx=1)\n",
      "    (embed_positions): XGLMSinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x XGLMDecoderLayer(\n",
      "        (self_attn): XGLMAttention(\n",
      "          (k_proj): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (v_proj): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (q_proj): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (out_proj): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "        (activation_fn): GELUActivation()\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (fc2): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=256008, bias=False)\n",
      ")\n",
      "Total trainable parameters in lora model : 3538944 and are 0.6269569729008008 % of the original model\n",
      "\n",
      "in getDataset\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading readme:   0%|          | 0.00/2.36k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ccb548682e2d40eaba5721c246e44368"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading metadata:   0%|          | 0.00/945 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "55aabcad62454c42baac31cbac97d63a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 11.7M/11.7M [00:02<00:00, 4.88MB/s]\n",
      "Downloading data: 100%|██████████| 1.46M/1.46M [00:00<00:00, 4.67MB/s]\n",
      "Downloading data: 100%|██████████| 1.45M/1.45M [00:00<00:00, 4.91MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Generating train split:   0%|          | 0/102747 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "82c6a3a23eb748768dd3f3be34274270"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating validation split:   0%|          | 0/12844 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "11fa372136c9427f92d3b89aa148487e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating test split:   0%|          | 0/12843 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a20268a5b14948358b60cbd17ccd22d4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['es', 'qu'],\n",
      "        num_rows: 102747\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['es', 'qu'],\n",
      "        num_rows: 12844\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['es', 'qu'],\n",
      "        num_rows: 12843\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/102747 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3cc89a5139fa41b3af96f5433b47b595"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4235 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/12844 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d5b1dfb1184745eea9d41bda4c20751c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/12843 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cd82eeaa38a249e3a57040e484250154"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map (num_proc=4):   0%|          | 0/102747 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e20f0b347d2a4c3ba8a5d7998ed3b225"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map (num_proc=4):   0%|          | 0/12844 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ebc17339463c41b1b6002ad7eb303d0a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map (num_proc=4):   0%|          | 0/12843 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9696ccca9d80466eab18b70304717261"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 80787\n",
      "})\n",
      "{'input_ids': [2, 4049, 39822, 27076, 2800, 3451, 27076, 7382, 106026, 129598, 2597, 6580, 10988, 81990, 78702, 247, 134073, 5, 78511, 1190, 21167, 133189, 78702, 116, 118, 42783, 162637, 80, 65704, 81990, 6606, 134073], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [2, 4049, 39822, 27076, 2800, 3451, 27076, 7382, 106026, 129598, 2597, 6580, 10988, 81990, 78702, 247, 134073, 5, 78511, 1190, 21167, 133189, 78702, 116, 118, 42783, 162637, 80, 65704, 81990, 6606, 134073]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33maleksey-morshnev\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.16.4 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.3"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/home/reni/Documents/hws_ws23/nnti_project/notebooks/wandb/run-20240317_054744-9b1uckr8</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/aleksey-morshnev/huggingface/runs/9b1uckr8' target=\"_blank\">soft-water-1</a></strong> to <a href='https://wandb.ai/aleksey-morshnev/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/aleksey-morshnev/huggingface' target=\"_blank\">https://wandb.ai/aleksey-morshnev/huggingface</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/aleksey-morshnev/huggingface/runs/9b1uckr8' target=\"_blank\">https://wandb.ai/aleksey-morshnev/huggingface/runs/9b1uckr8</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='30297' max='30297' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [30297/30297 1:28:16, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>3.599100</td>\n      <td>3.424140</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>3.419900</td>\n      <td>3.266045</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>3.320700</td>\n      <td>3.188933</td>\n    </tr>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training time : 5304.447983503342 sec.\n"
     ]
    }
   ],
   "source": [
    "train_XGLM_lora()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T06:16:07.872990Z",
     "start_time": "2024-03-17T04:46:57.341529Z"
    }
   },
   "id": "4552a2886513c1e1",
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
