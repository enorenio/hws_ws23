{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Language model inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal if this first task is to familiarize yourself with the huggingface transformers and dataset libraries. You will learn how to load and tokenize a dataset, how to load a pre-trained language model, and finally, how to run a model in inference mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to complete the missing code blocks below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset, load_dataset_builder, get_dataset_split_names, get_dataset_config_names\n",
    "from transformers import XGLMTokenizer, XGLMTokenizerFast, XGLMForCausalLM, AutoModelForCausalLM, AutoTokenizer, GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SET_NAME = \"facebook/flores\" # specify dataset name\n",
    "MODEL_NAME = \"facebook/xglm-564M\" # specify model name\n",
    "# MODEL_NAME = \"gpt2\" # specify model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore a dataset\n",
    "\n",
    "# covered language codes can be found here: https://github.com/openlanguagedata/flores?tab=readme-ov-file#language-coverage\n",
    "\n",
    "ds_builder = load_dataset_builder(\"facebook/flores\", \"deu_Latn\")\n",
    "print(ds_builder.info.description) # print the dataset description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the features (columns) of the dataset\n",
    "# TODO: your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the available splits\n",
    "# TODO: your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data, tokenize, and batchify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify languages\n",
    "LANGUAGES = [\n",
    "    \"eng_Latn\",\n",
    "    \"spa_Latn\",\n",
    "    \"ita_Latn\",\n",
    "    \"deu_Latn\",\n",
    "    \"arb_Arab\",\n",
    "    \"tel_Telu\",\n",
    "    \"tam_Taml\",\n",
    "    \"quy_Latn\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load flores data for each language\n",
    "# TODO: your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the English subset\n",
    "# TODO: your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at an individal sample from the dataset\n",
    "# TODO: your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the data\n",
    "\n",
    "# load a pre-trained tokenizer from the huggingface hub\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# gpt2 does not have a padding token, so we have to add it manually\n",
    "if MODEL_NAME == \"gpt2\":\n",
    "    tokenizer.add_special_tokens({'pad_token': tokenizer.unk_token})\n",
    "\n",
    "# specify the tokenization function\n",
    "def tokenization(example):\n",
    "    # fill in here\n",
    "    pass\n",
    "\n",
    "# TODO: your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a look at a tokenized sample\n",
    "# TODO: your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a pytorch data loader for each dataset\n",
    "BATCH_SIZE = 2 # for testing purposes, we start with a batch size of 2. You can change this later.\n",
    "\n",
    "# TODO: your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained model from the huggingface hub\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "# put the model into evaluation mode\n",
    "# TODO: your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {lang: [] for lang in LANGUAGES} # store per-batch losses for each language\n",
    "\n",
    "# iterate over the datset for each language and compute the cross-entropy loss per batch \n",
    "# TODO: your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize loss per language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a figure\n",
    "fig, axes = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "# create a bar plot for each langauge\n",
    "# TODO: your code goes here\n",
    "\n",
    "# format plot\n",
    "axes.set_xlabel(\"language\") # x-axis label\n",
    "axes.set_xticks(range(len(LANGUAGES))) # x-axis ticks\n",
    "axes.set_xticklabels(losses.keys()) # x-axis tick labels\n",
    "axes.set_ylabel(\"loss\") # y-axis label\n",
    "axes.set_ylim(0, 9) # range of y-axis\n",
    "axes.set_title(MODEL_NAME); # title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing XGLM to GPT2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your next task is to re-run the analysis above, but using `gpt2` as the pre-trained language model. For this exercise, focus on your native language, unless it's English or isn't covered by flores. In that case, pick another language that you can read well. \n",
    "\n",
    "Compare the language modeling loss of XGLM and GPT2. What do you observe? Investigate the differences in tokenization for XGLM and GPT2. What do you observe? How can the good (or bad) performance of GPT2 be explained?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: your code goes here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
