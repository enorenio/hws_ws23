{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Visualize hidden represenations of a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T00:27:30.789768Z",
     "start_time": "2024-03-14T00:27:30.785285Z"
    }
   },
   "outputs": [],
   "source": [
    "# import h5py\n",
    "# \n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T00:27:30.806374Z",
     "start_time": "2024-03-14T00:27:30.791727Z"
    }
   },
   "outputs": [],
   "source": [
    "packages_to_install = [\"ipywidgets\", \"numpy=1.24.0\", \"torch\", \"matplotlib\", \"sentencepiece\", \"protobuf\", \"datasets\", \"transformers\", \"diffusers\", \"peft\", \"h5py\", \"scikit-learn\", \"scipy\", \"wandb\"]"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 'conda-forge' already in 'channels' list, moving to the top\r\n"
     ]
    }
   ],
   "source": [
    "!conda config --add channels conda-forge"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T00:27:31.666300Z",
     "start_time": "2024-03-14T00:27:30.808042Z"
    }
   },
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipywidgets is already installed.\n",
      "numpy=1.24.0 is not installed. Installing it now...\n",
      "Collecting package metadata (current_repodata.json): - WARNING conda.models.version:get_matcher(556): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.7.1.*, but conda is ignoring the .* and treating it as 1.7.1\r\n",
      "done\r\n",
      "Solving environment: done\r\n",
      "\r\n",
      "\r\n",
      "==> WARNING: A newer version of conda exists. <==\r\n",
      "  current version: 23.7.4\r\n",
      "  latest version: 24.1.2\r\n",
      "\r\n",
      "Please update conda by running\r\n",
      "\r\n",
      "    $ conda update -n base -c conda-forge conda\r\n",
      "\r\n",
      "Or to minimize the number of packages updated during conda update use\r\n",
      "\r\n",
      "     conda install conda=24.1.2\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "# All requested packages already installed.\r\n",
      "\r\n",
      "torch is already installed.\n",
      "matplotlib is already installed.\n",
      "sentencepiece is already installed.\n",
      "protobuf is not installed. Installing it now...\n",
      "Collecting package metadata (current_repodata.json): / WARNING conda.models.version:get_matcher(556): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.7.1.*, but conda is ignoring the .* and treating it as 1.7.1\r\n",
      "done\r\n",
      "Solving environment: done\r\n",
      "\r\n",
      "\r\n",
      "==> WARNING: A newer version of conda exists. <==\r\n",
      "  current version: 23.7.4\r\n",
      "  latest version: 24.1.2\r\n",
      "\r\n",
      "Please update conda by running\r\n",
      "\r\n",
      "    $ conda update -n base -c conda-forge conda\r\n",
      "\r\n",
      "Or to minimize the number of packages updated during conda update use\r\n",
      "\r\n",
      "     conda install conda=24.1.2\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "# All requested packages already installed.\r\n",
      "\r\n",
      "datasets is already installed.\n",
      "transformers is already installed.\n",
      "diffusers is already installed.\n",
      "peft is already installed.\n",
      "h5py is not installed. Installing it now...\n",
      "Collecting package metadata (current_repodata.json): | WARNING conda.models.version:get_matcher(556): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.7.1.*, but conda is ignoring the .* and treating it as 1.7.1\r\n",
      "done\r\n",
      "Solving environment: done\r\n",
      "\r\n",
      "\r\n",
      "==> WARNING: A newer version of conda exists. <==\r\n",
      "  current version: 23.7.4\r\n",
      "  latest version: 24.1.2\r\n",
      "\r\n",
      "Please update conda by running\r\n",
      "\r\n",
      "    $ conda update -n base -c conda-forge conda\r\n",
      "\r\n",
      "Or to minimize the number of packages updated during conda update use\r\n",
      "\r\n",
      "     conda install conda=24.1.2\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "# All requested packages already installed.\r\n",
      "\r\n",
      "scikit-learn is not installed. Installing it now...\n",
      "Collecting package metadata (current_repodata.json): / WARNING conda.models.version:get_matcher(556): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.7.1.*, but conda is ignoring the .* and treating it as 1.7.1\r\n",
      "done\r\n",
      "Solving environment: done\r\n",
      "\r\n",
      "\r\n",
      "==> WARNING: A newer version of conda exists. <==\r\n",
      "  current version: 23.7.4\r\n",
      "  latest version: 24.1.2\r\n",
      "\r\n",
      "Please update conda by running\r\n",
      "\r\n",
      "    $ conda update -n base -c conda-forge conda\r\n",
      "\r\n",
      "Or to minimize the number of packages updated during conda update use\r\n",
      "\r\n",
      "     conda install conda=24.1.2\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "# All requested packages already installed.\r\n",
      "\r\n",
      "scipy is already installed.\n",
      "wandb is already installed.\n",
      "CPU times: user 5.79 s, sys: 1.81 s, total: 7.61 s\n",
      "Wall time: 2min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import importlib\n",
    "\n",
    "for package_name in packages_to_install:\n",
    "    try:\n",
    "        importlib.import_module(package_name)\n",
    "        print(f\"{package_name} is already installed.\")\n",
    "    except ImportError:\n",
    "        print(f\"{package_name} is not installed. Installing it now...\")\n",
    "        !conda install -y {package_name}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T00:29:47.711392Z",
     "start_time": "2024-03-14T00:27:31.668838Z"
    }
   },
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset, load_dataset_builder, get_dataset_split_names, get_dataset_config_names\n",
    "from transformers import XGLMTokenizer, XGLMTokenizerFast, XGLMForCausalLM, AutoModelForCausalLM, AutoTokenizer, GenerationConfig"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T00:29:48.075515Z",
     "start_time": "2024-03-14T00:29:47.713267Z"
    }
   },
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "DATA_SET_NAME = \"facebook/flores\"\n",
    "MODEL_NAME = \"facebook/xglm-564M\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LANGUAGES = [\n",
    "    \"eng_Latn\",\n",
    "    \"spa_Latn\",\n",
    "    \"ita_Latn\",\n",
    "    \"deu_Latn\",\n",
    "    \"arb_Arab\",\n",
    "    \"tel_Telu\",\n",
    "    \"tam_Taml\",\n",
    "    \"quy_Latn\"\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T00:29:48.111310Z",
     "start_time": "2024-03-14T00:29:48.076948Z"
    }
   },
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 923 ms, sys: 146 ms, total: 1.07 s\n",
      "Wall time: 19.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'eng_Latn': <datasets_modules.datasets.facebook--flores.2a1174c8c4991ca09a9cb5b9a367cb2e049b073852cb4097456164d4612391ef.flores.Flores200 at 0x7f5a2220f670>,\n 'spa_Latn': <datasets_modules.datasets.facebook--flores.2a1174c8c4991ca09a9cb5b9a367cb2e049b073852cb4097456164d4612391ef.flores.Flores200 at 0x7f5a1ce29270>,\n 'ita_Latn': <datasets_modules.datasets.facebook--flores.2a1174c8c4991ca09a9cb5b9a367cb2e049b073852cb4097456164d4612391ef.flores.Flores200 at 0x7f5a1ce28850>,\n 'deu_Latn': <datasets_modules.datasets.facebook--flores.2a1174c8c4991ca09a9cb5b9a367cb2e049b073852cb4097456164d4612391ef.flores.Flores200 at 0x7f5a1ce29540>,\n 'arb_Arab': <datasets_modules.datasets.facebook--flores.2a1174c8c4991ca09a9cb5b9a367cb2e049b073852cb4097456164d4612391ef.flores.Flores200 at 0x7f5a19a9c0d0>,\n 'tel_Telu': <datasets_modules.datasets.facebook--flores.2a1174c8c4991ca09a9cb5b9a367cb2e049b073852cb4097456164d4612391ef.flores.Flores200 at 0x7f5a19479060>,\n 'tam_Taml': <datasets_modules.datasets.facebook--flores.2a1174c8c4991ca09a9cb5b9a367cb2e049b073852cb4097456164d4612391ef.flores.Flores200 at 0x7f5a1949c820>,\n 'quy_Latn': <datasets_modules.datasets.facebook--flores.2a1174c8c4991ca09a9cb5b9a367cb2e049b073852cb4097456164d4612391ef.flores.Flores200 at 0x7f5a194b8310>}"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "language_flores_data = {}\n",
    "for language in LANGUAGES:\n",
    "    language_flores_data[language] = load_dataset_builder(DATA_SET_NAME, language, trust_remote_code=True)\n",
    "    language_flores_data[language].download_and_prepare()\n",
    "\n",
    "language_flores_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T00:30:07.586107Z",
     "start_time": "2024-03-14T00:29:48.113179Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    dev: Dataset({\n        features: ['id', 'URL', 'domain', 'topic', 'has_image', 'has_hyperlink', 'sentence'],\n        num_rows: 997\n    })\n    devtest: Dataset({\n        features: ['id', 'URL', 'domain', 'topic', 'has_image', 'has_hyperlink', 'sentence'],\n        num_rows: 1012\n    })\n})"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_dataset = language_flores_data[\"eng_Latn\"].as_dataset()\n",
    "eng_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T00:30:07.608542Z",
     "start_time": "2024-03-14T00:30:07.588030Z"
    }
   },
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# tokenize the data\n",
    "\n",
    "# load a pre-trained tokenizer from the huggingface hub\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# gpt2 does not have a padding token, so we have to add it manually\n",
    "if MODEL_NAME == \"gpt2\":\n",
    "    tokenizer.add_special_tokens({'pad_token': tokenizer.unk_token})\n",
    "\n",
    "# specify the tokenization function\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['sentence'])\n",
    "# TODO: your code goes here\n",
    "\n",
    "tokenized_datasets = {lang: language_flores_data[lang].as_dataset().map(tokenize_function, batched=True) for lang in LANGUAGES}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T00:30:10.072349Z",
     "start_time": "2024-03-14T00:30:07.609910Z"
    }
   },
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for key, data in tokenized_datasets.items():\n",
    "    tokenized_datasets[key] = tokenized_datasets[key].remove_columns([\"id\", \"URL\", \"domain\", \"topic\", \"has_image\", \"has_hyperlink\", \"sentence\"])\n",
    "    tokenized_datasets[key].set_format(\"torch\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T00:30:10.092124Z",
     "start_time": "2024-03-14T00:30:10.074726Z"
    }
   },
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'eng_Latn': {'dev': <torch.utils.data.dataloader.DataLoader at 0x7f5a1947ba60>,\n  'devtest': <torch.utils.data.dataloader.DataLoader at 0x7f5a1947bac0>},\n 'spa_Latn': {'dev': <torch.utils.data.dataloader.DataLoader at 0x7f5a194f1420>,\n  'devtest': <torch.utils.data.dataloader.DataLoader at 0x7f5a194f2050>},\n 'ita_Latn': {'dev': <torch.utils.data.dataloader.DataLoader at 0x7f5a041ae9b0>,\n  'devtest': <torch.utils.data.dataloader.DataLoader at 0x7f5a041ae410>},\n 'deu_Latn': {'dev': <torch.utils.data.dataloader.DataLoader at 0x7f5a041ae290>,\n  'devtest': <torch.utils.data.dataloader.DataLoader at 0x7f5a041af310>},\n 'arb_Arab': {'dev': <torch.utils.data.dataloader.DataLoader at 0x7f5a041af400>,\n  'devtest': <torch.utils.data.dataloader.DataLoader at 0x7f5a041af4f0>},\n 'tel_Telu': {'dev': <torch.utils.data.dataloader.DataLoader at 0x7f5a041af5e0>,\n  'devtest': <torch.utils.data.dataloader.DataLoader at 0x7f5a041af730>},\n 'tam_Taml': {'dev': <torch.utils.data.dataloader.DataLoader at 0x7f5a041ae5c0>,\n  'devtest': <torch.utils.data.dataloader.DataLoader at 0x7f5a041ae890>},\n 'quy_Latn': {'dev': <torch.utils.data.dataloader.DataLoader at 0x7f5a041af7c0>,\n  'devtest': <torch.utils.data.dataloader.DataLoader at 0x7f5a041aedd0>}}"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct a pytorch data loader for each dataset\n",
    "from typing import Dict\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "BATCH_SIZE = 2 # for testing purposes, we start with a batch size of 2. You can change this later.\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "flores_dataloaders: Dict[str, Dict[str, DataLoader]] = {}\n",
    "# Iterate over languages\n",
    "for language in LANGUAGES:\n",
    "    flores_dataloaders[language] = {}  # Initialize a dictionary for each language\n",
    "    # Iterate over data splits for the current language\n",
    "    for split_name, dataset in tokenized_datasets[language].items():\n",
    "        flores_dataloaders[language][split_name] = DataLoader(dataset, batch_size=BATCH_SIZE, collate_fn=data_collator)\n",
    "flores_dataloaders\n",
    "# TODO: your code goes here"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T00:30:10.562586Z",
     "start_time": "2024-03-14T00:30:10.093444Z"
    }
   },
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[     2,   1504,  28488,      4, 140003,    501,     32, 200884,   6073,\n",
      "           9512,     48,  88230,  76168,     32, 160597,     48,     11,    929,\n",
      "          55516,  35761,    155,    490,   9482,  89288,    235,   6950,     13,\n",
      "             11,  61368,  24049,   2005,  37295,    155,    490,    113, 213481,\n",
      "             72,   1117,   5885,  86368,   6929, 111288,      7,     73,  59298,\n",
      "            769,    743,    242,      5,    211,      5,  19015,   5129,      5],\n",
      "        [     2, 106920, 169914,   2492,    319,   1246,  13470,  15170,     10,\n",
      "         178985,     48,  21558,      4, 138365,  73517,    219,      4,  31766,\n",
      "             53,   1253,   5940,     33,  45526,     22,  12740,      8,     95,\n",
      "          41440,  33504,      4,   2743,     32, 150346,  73401,     73, 102498,\n",
      "             88,   3234,    157,  74077,  21558,    490,    113,  11135,   2947,\n",
      "             48,  34300,     64,  33504,      5,      1,      1,      1,      1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0, 0]]), 'labels': tensor([[     2,   1504,  28488,      4, 140003,    501,     32, 200884,   6073,\n",
      "           9512,     48,  88230,  76168,     32, 160597,     48,     11,    929,\n",
      "          55516,  35761,    155,    490,   9482,  89288,    235,   6950,     13,\n",
      "             11,  61368,  24049,   2005,  37295,    155,    490,    113, 213481,\n",
      "             72,   1117,   5885,  86368,   6929, 111288,      7,     73,  59298,\n",
      "            769,    743,    242,      5,    211,      5,  19015,   5129,      5],\n",
      "        [     2, 106920, 169914,   2492,    319,   1246,  13470,  15170,     10,\n",
      "         178985,     48,  21558,      4, 138365,  73517,    219,      4,  31766,\n",
      "             53,   1253,   5940,     33,  45526,     22,  12740,      8,     95,\n",
      "          41440,  33504,      4,   2743,     32, 150346,  73401,     73, 102498,\n",
      "             88,   3234,    157,  74077,  21558,    490,    113,  11135,   2947,\n",
      "             48,  34300,     64,  33504,      5,   -100,   -100,   -100,   -100]])}\n"
     ]
    }
   ],
   "source": [
    "for i in flores_dataloaders['eng_Latn']['dev']:\n",
    "    print(i)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T00:30:10.592599Z",
     "start_time": "2024-03-14T00:30:10.564042Z"
    }
   },
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "counter_file = \"experiment_counter.txt\"\n",
    "experimentCounter = 0\n",
    "\n",
    "# Check if counter file exists\n",
    "if os.path.exists(counter_file):\n",
    "    # Read counter from file\n",
    "    with open(counter_file, 'r') as f:\n",
    "        experimentCounter = int(f.read())\n",
    "\n",
    "filename = f\"{experimentCounter}_hidden_representations.hdf5\"\n",
    "\n",
    "# Increment counter\n",
    "experimentCounter += 1\n",
    "with open(counter_file, 'w') as f:\n",
    "    f.write(str(experimentCounter))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T00:33:29.376562Z",
     "start_time": "2024-03-14T00:33:29.371796Z"
    }
   },
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/reni/miniconda3/envs/hws_ws23/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "text/plain": "XGLMForCausalLM(\n  (model): XGLMModel(\n    (embed_tokens): Embedding(256008, 1024, padding_idx=1)\n    (embed_positions): XGLMSinusoidalPositionalEmbedding()\n    (layers): ModuleList(\n      (0-23): 24 x XGLMDecoderLayer(\n        (self_attn): XGLMAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (activation_fn): GELUActivation()\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=1024, out_features=256008, bias=False)\n)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T00:34:56.118449Z",
     "start_time": "2024-03-14T00:34:52.883519Z"
    }
   },
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available, else fallback to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T00:34:59.633125Z",
     "start_time": "2024-03-14T00:34:59.626960Z"
    }
   },
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def add_groups(name, f: h5py.File) -> h5py.Group:\n",
    "    return f.create_group(name)\n",
    "\n",
    "def add_dataset(data, name, grp: h5py.Group):\n",
    "    grp.create_dataset(name,  data=data.cpu().detach().numpy())\n",
    "\n",
    "def add_token_rep(tokens, data, grp, key, sen_num, layer_no):\n",
    "    for idx, token in enumerate(tokens):\n",
    "        if token != '<pad>':\n",
    "            add_dataset(data[idx], f'token_{token}_{idx}', grp)\n",
    "\n",
    "def add_sentence_rep(f, sentence, data):\n",
    "    try:\n",
    "        grp = add_groups('full_sentence_encoding', f)\n",
    "    except:\n",
    "        pass\n",
    "    add_dataset(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%%time\n",
    "import h5py\n",
    "losses = {lang: [] for lang in LANGUAGES} # store per-batch losses for each language\n",
    "\n",
    "# Check if GPU is available, else fallback to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(device)\n",
    "\n",
    "f = h5py.File(filename,'a')\n",
    "\n",
    "for lang, loaders in flores_dataloaders.items():\n",
    "    print(lang)\n",
    "    try:\n",
    "        lang_grp = add_groups(lang, f)\n",
    "    except:\n",
    "        print(\"Exception 1\")\n",
    "        pass\n",
    "    for split, loader in loaders.items():\n",
    "        try:\n",
    "            type_grp = add_groups(split, lang_grp)\n",
    "        except:\n",
    "            print(\"Exception 2\")\n",
    "            pass\n",
    "        for batch in loader:\n",
    "            # for key, value in batch.items():\n",
    "            #   print(key, value)\n",
    "            with torch.no_grad():\n",
    "                inputs = batch.to(device=device)\n",
    "                outputs = model(**inputs)\n",
    "                loss = outputs.loss.cpu()\n",
    "                losses[lang].append(loss)\n",
    "                print(loss.item())\n",
    "        print(f\"Finished losses for {lang}\")\n",
    "\n",
    "# iterate over the datset for each language and compute the cross-entropy loss per batch\n",
    "# TODO: your code goes here"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
