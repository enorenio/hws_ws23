{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Task: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "packages_to_install = {\n",
    "    \"numpy\": \"numpy=1.24.0\",\n",
    "    \"matplotlib\": \"matplotlib\",\n",
    "    \"scikit-learn\": \"-c conda-forge scikit-learn\",\n",
    "    \"scipy\": \"-c conda-forge scipy\",\n",
    "    \"pandas\": \"pandas\"\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "import importlib\n",
    "\n",
    "for package_name, install_command in packages_to_install.items():\n",
    "    try:\n",
    "        importlib.import_module(package_name)\n",
    "        print(f\"{package_name} is already installed.\")\n",
    "    except ImportError:\n",
    "        print(f\"{package_name} is not installed. Installing it now...\")\n",
    "        !conda install -y {install_command}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T06:37:09.878840Z",
     "start_time": "2023-12-14T06:37:06.786210Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, KFold, LeaveOneOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "target = raw_df.values[1::2, 2]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T06:38:01.975208Z",
     "start_time": "2023-12-14T06:38:01.487489Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15. ,\n       18.9, 21.7, 20.4, 18.2, 19.9, 23.1, 17.5, 20.2, 18.2, 13.6, 19.6,\n       15.2, 14.5, 15.6, 13.9, 16.6, 14.8, 18.4, 21. , 12.7, 14.5, 13.2,\n       13.1, 13.5, 18.9, 20. , 21. , 24.7, 30.8, 34.9, 26.6, 25.3, 24.7,\n       21.2, 19.3, 20. , 16.6, 14.4, 19.4, 19.7, 20.5, 25. , 23.4, 18.9,\n       35.4, 24.7, 31.6, 23.3, 19.6, 18.7, 16. , 22.2, 25. , 33. , 23.5,\n       19.4, 22. , 17.4, 20.9, 24.2, 21.7, 22.8, 23.4, 24.1, 21.4, 20. ,\n       20.8, 21.2, 20.3, 28. , 23.9, 24.8, 22.9, 23.9, 26.6, 22.5, 22.2,\n       23.6, 28.7, 22.6, 22. , 22.9, 25. , 20.6, 28.4, 21.4, 38.7, 43.8,\n       33.2, 27.5, 26.5, 18.6, 19.3, 20.1, 19.5, 19.5, 20.4, 19.8, 19.4,\n       21.7, 22.8, 18.8, 18.7, 18.5, 18.3, 21.2, 19.2, 20.4, 19.3, 22. ,\n       20.3, 20.5, 17.3, 18.8, 21.4, 15.7, 16.2, 18. , 14.3, 19.2, 19.6,\n       23. , 18.4, 15.6, 18.1, 17.4, 17.1, 13.3, 17.8, 14. , 14.4, 13.4,\n       15.6, 11.8, 13.8, 15.6, 14.6, 17.8, 15.4, 21.5, 19.6, 15.3, 19.4,\n       17. , 15.6, 13.1, 41.3, 24.3, 23.3, 27. , 50. , 50. , 50. , 22.7,\n       25. , 50. , 23.8, 23.8, 22.3, 17.4, 19.1, 23.1, 23.6, 22.6, 29.4,\n       23.2, 24.6, 29.9, 37.2, 39.8, 36.2, 37.9, 32.5, 26.4, 29.6, 50. ,\n       32. , 29.8, 34.9, 37. , 30.5, 36.4, 31.1, 29.1, 50. , 33.3, 30.3,\n       34.6, 34.9, 32.9, 24.1, 42.3, 48.5, 50. , 22.6, 24.4, 22.5, 24.4,\n       20. , 21.7, 19.3, 22.4, 28.1, 23.7, 25. , 23.3, 28.7, 21.5, 23. ,\n       26.7, 21.7, 27.5, 30.1, 44.8, 50. , 37.6, 31.6, 46.7, 31.5, 24.3,\n       31.7, 41.7, 48.3, 29. , 24. , 25.1, 31.5, 23.7, 23.3, 22. , 20.1,\n       22.2, 23.7, 17.6, 18.5, 24.3, 20.5, 24.5, 26.2, 24.4, 24.8, 29.6,\n       42.8, 21.9, 20.9, 44. , 50. , 36. , 30.1, 33.8, 43.1, 48.8, 31. ,\n       36.5, 22.8, 30.7, 50. , 43.5, 20.7, 21.1, 25.2, 24.4, 35.2, 32.4,\n       32. , 33.2, 33.1, 29.1, 35.1, 45.4, 35.4, 46. , 50. , 32.2, 22. ,\n       20.1, 23.2, 22.3, 24.8, 28.5, 37.3, 27.9, 23.9, 21.7, 28.6, 27.1,\n       20.3, 22.5, 29. , 24.8, 22. , 26.4, 33.1, 36.1, 28.4, 33.4, 28.2,\n       22.8, 20.3, 16.1, 22.1, 19.4, 21.6, 23.8, 16.2, 17.8, 19.8, 23.1,\n       21. , 23.8, 23.1, 20.4, 18.5, 25. , 24.6, 23. , 22.2, 19.3, 22.6,\n       19.8, 17.1, 19.4, 22.2, 20.7, 21.1, 19.5, 18.5, 20.6, 19. , 18.7,\n       32.7, 16.5, 23.9, 31.2, 17.5, 17.2, 23.1, 24.5, 26.6, 22.9, 24.1,\n       18.6, 30.1, 18.2, 20.6, 17.8, 21.7, 22.7, 22.6, 25. , 19.9, 20.8,\n       16.8, 21.9, 27.5, 21.9, 23.1, 50. , 50. , 50. , 50. , 50. , 13.8,\n       13.8, 15. , 13.9, 13.3, 13.1, 10.2, 10.4, 10.9, 11.3, 12.3,  8.8,\n        7.2, 10.5,  7.4, 10.2, 11.5, 15.1, 23.2,  9.7, 13.8, 12.7, 13.1,\n       12.5,  8.5,  5. ,  6.3,  5.6,  7.2, 12.1,  8.3,  8.5,  5. , 11.9,\n       27.9, 17.2, 27.5, 15. , 17.2, 17.9, 16.3,  7. ,  7.2,  7.5, 10.4,\n        8.8,  8.4, 16.7, 14.2, 20.8, 13.4, 11.7,  8.3, 10.2, 10.9, 11. ,\n        9.5, 14.5, 14.1, 16.1, 14.3, 11.7, 13.4,  9.6,  8.7,  8.4, 12.8,\n       10.5, 17.1, 18.4, 15.4, 10.8, 11.8, 14.9, 12.6, 14.1, 13. , 13.4,\n       15.2, 16.1, 17.8, 14.9, 14.1, 12.7, 13.5, 14.9, 20. , 16.4, 17.7,\n       19.5, 20.2, 21.4, 19.9, 19. , 19.1, 19.1, 20.1, 19.9, 19.6, 23.2,\n       29.8, 13.8, 13.3, 16.7, 12. , 14.6, 21.4, 23. , 23.7, 25. , 21.8,\n       20.6, 21.2, 19.1, 20.6, 15.2,  7. ,  8.1, 13.6, 20.1, 21.8, 24.5,\n       23.1, 19.7, 18.3, 21.2, 17.5, 16.8, 22.4, 20.6, 23.9, 22. , 11.9])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T06:42:10.637773Z",
     "start_time": "2023-12-14T06:42:10.617566Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a code skeleton for performing linear regression. \n",
    "Your task is to complete the functions where required. \n",
    "You are only allowed to use built-in Python functions, as well as any `numpy` functions. No other libraries / imports are allowed. \n",
    "\n",
    "In the beginning of every function there is docstring which specifies the input and and expected output.\n",
    "Write your code in a way that adheres to it.\n",
    "You may only use plain python and anything that we imported for you above such as numpy functions (i.e. no other scikit-learn classifiers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting the results to PDF\n",
    "Once you complete the assignments, export the entire notebook as PDF and attach it to your homework solutions. \n",
    "The best way of doing that is\n",
    "1. Run all the cells of the notebook (`Kernel -> Restart & Run All`)\n",
    "2. Export/download the notebook as PDF (`File -> Download as -> PDF via LaTeX (.pdf)`)\n",
    "3. Concatenate your solutions for other tasks with the output of Step 2. On Linux you can simply use `pdfunite`, there are similar tools for other platforms too. You can only upload a single PDF file to Moodle.\n",
    "\n",
    "**Make sure** you are using `nbconvert` **Version 5.5 or later** by running `jupyter nbconvert --version`. Older versions clip lines that exceed page width, which makes your code harder to grade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment we will work with the Boston Housing Dataset.\n",
    "The data consists of 506 samples. Each sample represents a district in the city of Boston and has 13 features, such as crime rate or taxation level. The regression target is the median house price in the given district (in $1000's).\n",
    "\n",
    "More details can be found here: http://lib.stat.cmu.edu/datasets/boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X , y = load_boston(return_X_y=True)\n",
    "\n",
    "# Add a vector of ones to the data matrix to absorb the bias term\n",
    "# (Recall slide #7 from the lecture)\n",
    "X = np.hstack([np.ones([X.shape[0], 1]), X])\n",
    "# From now on, D refers to the number of features in the AUGMENTED dataset (i.e. including the dummy '1' feature for the absorbed bias term)\n",
    "\n",
    "# Split into train and test\n",
    "test_size = 0.2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Fit standard linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_least_squares(X, y):\n",
    "    \"\"\"Fit ordinary least squares model to the data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape [N, D]\n",
    "        (Augmented) feature matrix.\n",
    "    y : array, shape [N]\n",
    "        Regression targets.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    w : array, shape [D]\n",
    "        Optimal regression coefficients (w[0] is the bias term).\n",
    "        \n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION ###\n",
    "    return \n",
    "    ### END SOLUTION ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Fit ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ridge(X, y, reg_strength):\n",
    "    \"\"\"Fit ridge regression model to the data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape [N, D]\n",
    "        (Augmented) feature matrix.\n",
    "    y : array, shape [N]\n",
    "        Regression targets.\n",
    "    reg_strength : float\n",
    "        L2 regularization strength (denoted by lambda in the lecture)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    w : array, shape [D]\n",
    "        Optimal regression coefficients (w[0] is the bias term).\n",
    "    \n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION ###\n",
    "    return \n",
    "    ### END SOLUTION ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Generate predictions for new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_linear_model(X, w):\n",
    "    \"\"\"Generate predictions for the given samples.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape [N, D]\n",
    "        (Augmented) feature matrix.\n",
    "    w : array, shape [D]\n",
    "        Regression coefficients.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    y_pred : array, shape [N]\n",
    "        Predicted regression targets for the input data.\n",
    "        \n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION ###\n",
    "    return \n",
    "    ### END SOLUTION ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    \"\"\"Compute mean squared error between true and predicted regression targets.\n",
    "    \n",
    "    Reference: `https://en.wikipedia.org/wiki/Mean_squared_error`\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array\n",
    "        True regression targets.\n",
    "    y_pred : array\n",
    "        Predicted regression targets.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    mse : float\n",
    "        Mean squared error.\n",
    "        \n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION ###\n",
    "    return \n",
    "    ### END SOLUTION ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the two models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reference implementation produces for test size 0.2\n",
    "* MSE for Least squares $\\approx$ **23.96**\n",
    "* MSE for Ridge regression $\\approx$ **21.03**\n",
    "\n",
    "You results might be slightly (i.e. $\\pm 1\\%$) different from the reference solution due to numerical reasons. There are no tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "np.random.seed(1234)\n",
    "test_size=0.2\n",
    "X , y = load_boston(return_X_y=True)\n",
    "X = np.hstack([np.ones([X.shape[0], 1]), X])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "\n",
    "# Ordinary least squares regression\n",
    "w_ls = fit_least_squares(X_train, y_train)\n",
    "y_pred_ls = predict_linear_model(X_test, w_ls)\n",
    "mse_ls = mean_squared_error(y_test, y_pred_ls)\n",
    "print('MSE for Least squares = {0}'.format(mse_ls))\n",
    "\n",
    "# Ridge regression\n",
    "reg_strength = 1\n",
    "w_ridge = fit_ridge(X_train, y_train, reg_strength)\n",
    "y_pred_ridge = predict_linear_model(X_test, w_ridge)\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "print('MSE for Ridge regression = {0}'.format(mse_ridge))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Compare sklearn and numpy Kfold "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below sklearn K-fold Cross validation reference implementation is for you to read. For k=10 it produces\n",
    "* MSE for Least squares $\\approx$ **25.19**\n",
    "* MSE for Ridge regression $\\approx$ **26.70**\n",
    "\n",
    "You results might be slightly (i.e. $\\pm 0.1\\%$) different from the reference solution due to numerical reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_compare(n_folds):\n",
    "    np.random.seed(1234)\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=1234)\n",
    "\n",
    "    mse_ls_sum = 0\n",
    "    mse_ridge_sum = 0\n",
    "    X , y = load_boston(return_X_y=True)\n",
    "    X = np.hstack([np.ones([X.shape[0], 1]), X])\n",
    "    test_size = 0.2\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "        y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "\n",
    "        # Ordinary least squares regression\n",
    "        w_ls = fit_least_squares(X_train_fold, y_train_fold)\n",
    "        y_pred_ls = predict_linear_model(X_val_fold, w_ls)\n",
    "        mse_ls_fold = mean_squared_error(y_val_fold, y_pred_ls)\n",
    "        mse_ls_sum += mse_ls_fold\n",
    "\n",
    "        # Ridge regression\n",
    "        reg_strength = 1\n",
    "        w_ridge = fit_ridge(X_train_fold, y_train_fold, reg_strength)\n",
    "        y_pred_ridge = predict_linear_model(X_val_fold, w_ridge)\n",
    "        mse_ridge_fold = mean_squared_error(y_val_fold, y_pred_ridge)\n",
    "        mse_ridge_sum += mse_ridge_fold\n",
    "\n",
    "    # Calculate the average MSE over all folds\n",
    "    avg_mse_ls = mse_ls_sum / n_folds\n",
    "    avg_mse_ridge = mse_ridge_sum / n_folds\n",
    "\n",
    "    #print('Average MSE for Least squares = {0}'.format(avg_mse_ls))\n",
    "    #print('Average MSE for Ridge regression = {0}'.format(avg_mse_ridge))\n",
    "    return avg_mse_ls, avg_mse_ridge\n",
    "\n",
    "kfold_compare(n_folds=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct custom numpy implementation produces for 10-fold cross validation\n",
    "* MSE for Least squares $\\approx$ **24.91**\n",
    "* MSE for Ridge regression $\\approx$ **26.66**\n",
    "\n",
    "You results might be slightly (i.e. $\\pm 0.1\\%$) different from the reference solution due to numerical reasons. \n",
    "\n",
    "**Your task is to fill in the code to compute indices of subsets for validation fold and training fold, i.e. val_indices, train_indices**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_kfold_compare(n_folds):\n",
    "    np.random.seed(1234)\n",
    "\n",
    "    mse_ls_sum = 0\n",
    "    mse_ridge_sum = 0\n",
    "    \n",
    "    X , y = load_boston(return_X_y=True)\n",
    "    X = np.hstack([np.ones([X.shape[0], 1]), X])\n",
    "    test_size = 0.2\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    \n",
    "    n_samples = X_train.shape[0]\n",
    "    fold_size = n_samples // n_folds\n",
    "\n",
    "    for i in range(n_folds):\n",
    "        val_indices, train_indices = [], []\n",
    "        ### BEGIN SOLUTION ####\n",
    "\n",
    "        val_indices = _\n",
    "        train_indices = _\n",
    "        ### END SOLUTION ####\n",
    "\n",
    "        X_train_fold = X_train[train_indices]\n",
    "        X_val_fold = X_train[val_indices]\n",
    "        y_train_fold = y_train[train_indices]\n",
    "        y_val_fold = y_train[val_indices]\n",
    "\n",
    "        # Ordinary least squares regression\n",
    "        w_ls = fit_least_squares(X_train_fold, y_train_fold)\n",
    "        y_pred_ls = predict_linear_model(X_val_fold, w_ls)\n",
    "        mse_ls_fold = mean_squared_error(y_val_fold, y_pred_ls)\n",
    "        mse_ls_sum += mse_ls_fold\n",
    "\n",
    "        # Ridge regression\n",
    "        reg_strength = 1\n",
    "        w_ridge = fit_ridge(X_train_fold, y_train_fold, reg_strength)\n",
    "        y_pred_ridge = predict_linear_model(X_val_fold, w_ridge)\n",
    "        mse_ridge_fold = mean_squared_error(y_val_fold, y_pred_ridge)\n",
    "        mse_ridge_sum += mse_ridge_fold\n",
    "\n",
    "    # Calculate the average MSE over all folds\n",
    "    avg_mse_ls = mse_ls_sum / n_folds\n",
    "    avg_mse_ridge = mse_ridge_sum / n_folds\n",
    "\n",
    "    return avg_mse_ls, avg_mse_ridge\n",
    "\n",
    "# Call the function with n_folds=10 and print the results\n",
    "custom_kfold_compare(n_folds=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Compare different k fold cross validation\n",
    "Plot K-fold Cross Validation MSE for Least Squares Vs Rigde Regression for k in {2,3,4,..,10} using custom numpy implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(n_folds_values, results_ls,results_ridge):\n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(n_folds_values, results_ls, label='Least Squares', marker='o')\n",
    "    plt.plot(n_folds_values, results_ridge, label='Ridge Regression', marker='o')\n",
    "    plt.xlabel('Number of Folds (k)')\n",
    "    plt.ylabel('Average Mean Squared Error (MSE)')\n",
    "    plt.title('Average MSE vs. Number of Folds')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    return \n",
    "\n",
    "n_folds_values = range(2,11,1)\n",
    "\n",
    "### BEGIN SOLUTION ###\n",
    "results_ls = []\n",
    "results_ridge = []\n",
    "\n",
    "### END SOLUTION ###\n",
    "\n",
    "plot_comparison(n_folds_values, results_ls, results_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Leave One Out Cross Validation\n",
    "The below sklearn reference implementation for leave one out cross validation is for you to read. It produces\n",
    "* MSE for Least squares $\\approx$ **23.72**\n",
    "* MSE for Ridge regression $\\approx$ **24.48**\n",
    "\n",
    "You results might be slightly (i.e. $\\pm 0.1\\%$) different from the reference solution due to numerical reasons.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X , y = load_boston(return_X_y=True)\n",
    "X = np.hstack([np.ones([X.shape[0], 1]), X])\n",
    "\n",
    "# Initialize lists to store MSE values\n",
    "mse_ls_values = []\n",
    "mse_ridge_values = []\n",
    "\n",
    "# Create LOOCV iterator\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "# Iterate through LOOCV splits\n",
    "for train_index, test_index in loo.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Ordinary least squares regression\n",
    "    w_ls = fit_least_squares(X_train, y_train)\n",
    "    y_pred_ls = predict_linear_model(X_test, w_ls)\n",
    "    mse_ls = mean_squared_error(y_test, y_pred_ls)\n",
    "    mse_ls_values.append(mse_ls)\n",
    "\n",
    "    # Ridge regression\n",
    "    reg_strength = 1\n",
    "    w_ridge = fit_ridge(X_train, y_train, reg_strength)\n",
    "    y_pred_ridge = predict_linear_model(X_test, w_ridge)\n",
    "    mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "    mse_ridge_values.append(mse_ridge)\n",
    "\n",
    "# Calculate the average MSE over all LOOCV iterations\n",
    "avg_mse_ls_sk = np.mean(mse_ls_values)\n",
    "avg_mse_ridge_sk = np.mean(mse_ridge_values)\n",
    "\n",
    "print('Average MSE for Least squares (LOOCV) = {0}'.format(avg_mse_ls_sk))\n",
    "print('Average MSE for Ridge regression (LOOCV) = {0}'.format(avg_mse_ridge_sk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct custom numpy implementation produces for leave one out cross validation produces\n",
    "* MSE for Least squares $\\approx$ **23.72**\n",
    "* MSE for Ridge regression $\\approx$ **24.48**\n",
    "\n",
    "You results might be slightly (i.e. $\\pm 0.1\\%$) different from the reference solution due to numerical reasons. \n",
    "\n",
    "**Your task is to fill in the code to compute subsets for validation fold and training fold, i.e. (X_train,y_train) and (X_val,y_val)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store MSE values\n",
    "mse_ls_values = []\n",
    "mse_ridge_values = []\n",
    "\n",
    "# Leave-One-Out Cross-Validation (LOOCV)\n",
    "n = X.shape[0]\n",
    "\n",
    "for i in range(n):\n",
    "    ### BEGIN SOLUTION ###\n",
    "    X_train, y_train = _ , _\n",
    "    X_val, y_val = _ , _\n",
    "    ### END SOLUTION ###\n",
    "    \n",
    "    # Ordinary least squares regression\n",
    "    w_ls = fit_least_squares(X_train, y_train)\n",
    "    y_pred_ls = predict_linear_model(X_val, w_ls)\n",
    "    mse_ls = mean_squared_error(y_val, y_pred_ls)\n",
    "    mse_ls_values.append(mse_ls)\n",
    "\n",
    "    # Ridge regression\n",
    "    reg_strength = 1\n",
    "    w_ridge = fit_ridge(X_train, y_train, reg_strength)\n",
    "    y_pred_ridge = predict_linear_model(X_val, w_ridge)\n",
    "    mse_ridge = mean_squared_error(y_val, y_pred_ridge)\n",
    "    mse_ridge_values.append(mse_ridge)\n",
    "\n",
    "# Calculate the average MSE over all LOOCV iterations\n",
    "avg_mse_ls = np.mean(mse_ls_values)\n",
    "avg_mse_ridge = np.mean(mse_ridge_values)\n",
    "\n",
    "print('Average MSE for Least squares (LOOCV) = {0}'.format(avg_mse_ls))\n",
    "print('Average MSE for Ridge regression (LOOCV) = {0}'.format(avg_mse_ridge))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "48px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
