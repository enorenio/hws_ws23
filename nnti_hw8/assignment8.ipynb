{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team members\n",
    "Name:   \n",
    "Matrikelnummer:  \n",
    "email:  \n",
    "\n",
    "Name:   \n",
    "Matrikelnummer:  \n",
    "email:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8.4 (3.5 points)\n",
    "\n",
    "The goal of this notebook is to explore the effect of various regularization strategies on the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement all of your code in the assignment8.py file.\n",
    "Submit the notebook with output and the python script.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.transforms import transforms, v2\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from assignment8 import training_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dataset(dataset, rng, num_samples=100) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Reduces the MNIST dataset to have only num_samples for each class.\n",
    "    Returns the indices of all datapoints that should be kept\n",
    "    \"\"\"\n",
    "    reduced_indices = []\n",
    "    for target in range(10):\n",
    "        mask = dataset.targets == target\n",
    "        indices = np.nonzero(mask).flatten()\n",
    "        random_subset = rng.choice(indices, size=num_samples, replace=False)\n",
    "        reduced_indices.extend(random_subset)\n",
    "\n",
    "    return reduced_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4.1 Model implementation (0.5 pts)\n",
    "\n",
    "Complete the `FFNN` class, by implementing a simple feed-forward neural network with one hidden layer of size $N$ before predicting class *logits*.\n",
    "The network should use ReLU activation functions and Batch Normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4.2 Training & Evaluation (1 pt)\n",
    "In this task we will implement training and validation for one epoch. Complete the `train`and `validate` functions by training the model on all batches in the dataloader and calculating the cross entropy loss and the accuracy (percentage of correct classes) of the model.\n",
    "The functions should return the average loss and accuracy.\n",
    "Make sure to put the model into the correct mode before training/evaluating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data loading & Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THESE CELLS\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "normalize_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "normalized_mnist = MNIST(root=\"./data\", download=True, train=True, transform=normalize_transform)\n",
    "tiny_mnist_indices = reduce_dataset(normalized_mnist, rng=rng, num_samples=100)\n",
    "standard_train_set = Subset(normalized_mnist, tiny_mnist_indices)\n",
    "test_set = MNIST(root=\"./data\", download=True, train=False, transform=normalize_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(dataset):\n",
    "    fig, axes = plt.subplots(10, 10)\n",
    "    fig.set_size_inches(10, 6)\n",
    "    \n",
    "    for target in range(10):\n",
    "        target_subset = dataset.data[dataset.targets == target]\n",
    "        indices = np.random.choice(np.arange(len(target_subset)), 10)\n",
    "        example_images = target_subset.data[indices, :, :]\n",
    "        for i, image in enumerate(example_images):\n",
    "            axes[target, i].imshow(image)\n",
    "            axes[target, i].set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4.3 Training (1.5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the `training_loop` function by integrating the `FFNN` model, developed in task 1, and the `train` and `validate` functions\n",
    "from task 2.\n",
    "Use SGD as the optimizer with optional weight decay.\n",
    "Employ early stopping: if `early_stopping_patience` is greater than 0, terminate the training when there's no improvement on the validation set for a duration exceeding `early_stopping_patience` epochs.\n",
    "It's crucial to save model parameters ('checkpoints') upon achieving better validation set performance. In the case of early stopping, revert to the parameters from the last checkpoint. Note: For parameter storage, ensure to use deepcopy since PyTorch's default behavior is to return a reference to the weights, not a copy (refer to [PyTorch documentation](https://pytorch.org/tutorials/beginner/saving_loading_models.html)).\n",
    "\n",
    "Upon completion of training, graphically represent the loss and accuracy trends for both training and test datasets.\n",
    "\n",
    "The function should return the weights of the best checkpoint and the validation loss at that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = training_loop(\n",
    "    standard_train_set, test_set, \n",
    "    epochs=EPOCHS, \n",
    "    batch_size=32,\n",
    "    learning_rate=LEARNING_RATE, \n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    early_stopping_patience=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4.4 Data Augmentation (0.5 pts)\n",
    "In the previous tasks we have explored the effects of weight decay and early stopping on a tiny subset of MNIST. \n",
    "In this task we will explore if data augmentation can help improve the accuracy of our small dataset. You are encouraged to play\n",
    "around with the effects of various transformations on the result.\n",
    "\n",
    "Use transformations from PyTorch's `torchvision` [package](https://pytorch.org/vision/stable/transforms.html) to augment the dataset. You are also allowed to play around with the other hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    # Add your transformations here\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THESE CELLS\n",
    "augmented_mnist = MNIST(root=\"./data\", download=True, train=True, transform=augment_transform)\n",
    "augmented_train_set = Subset(augmented_mnist, tiny_mnist_indices)\n",
    "test_set = MNIST(root=\"./data\", download=True, train=False, transform=normalize_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = training_loop(\n",
    "    augmented_train_set, test_set, \n",
    "    epochs=EPOCHS, \n",
    "    batch_size=32,   # Larger dataset\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY, \n",
    "    early_stopping_patience=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
