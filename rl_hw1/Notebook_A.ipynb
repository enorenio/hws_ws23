{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise A2 (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finishing his undergrad studies, your good friend *Dieter Schlau* took up a job as a sysadmin. Now he reaches out to you for help with a sequential decision-making problem that involves network maintenance. In this notebook you will mainly implement a simulator/reinforcement learning environment for his problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## README\n",
    "We strongly recommend you install `gymnasium>=0.28.1` and `python>=3.10.*`. You can set up a python environment using e.g. [conda](https://docs.conda.io/projects/miniconda/en/latest/) in the terminal\n",
    "```\n",
    "conda create -n rl23 python=3.10 gymnasium=0.28.1 notebook -c conda-forge\n",
    "conda activate rl23\n",
    "```\n",
    "The `notebook` package is required for opening and working with this jupyter notebook.\n",
    "In a terminal with the Python environment active, run\n",
    "```\n",
    "jupyter notebook Notebook_A.ipynb\n",
    "```\n",
    "\n",
    "If you need help setting up a development environment, please visit the office hour or ask for help in the forum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "**Solve task (I), (II), and (III) and submit your final `.ipynb` file as a solution to the mCMS.**\n",
    "\n",
    "During grading, we will clear outputs and then run all cells.\n",
    "Notebooks that produce runtime errors will be graded with 0 points for Task (II) and (III), unless the errors arise due to gymnasium/numpy backwards compatibility issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Callable\n",
    "from dataclasses import dataclass\n",
    "from functools import cached_property, partial\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem statement\n",
    "#### Network topology\n",
    "The sequential decision making problem Dieter needs help solving, involves maintaining a network of $n > 1$ **servers (nodes)**,\n",
    "connected through (undirected) edges denoted by $c_{ij} = c_{ji} = 1\\{i \\text{ and } j \\text{ are connected}\\}$.\n",
    "These **fixed** connections arise from the fact the servers depend on each other to some extent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SysadminInstance:\n",
    "    \"\"\"\n",
    "    A class that captures the concrete network instance, i.e.\n",
    "    the number of servers (n), the connections (c_{ij})\n",
    "    and some values for the parameters alpha and epsilon\n",
    "    that are discussed below.\n",
    "    \"\"\"\n",
    "    num_servers: int\n",
    "    connections: list[tuple[int, int]]\n",
    "    alpha: float\n",
    "    eps: float\n",
    "   \n",
    "    @cached_property\n",
    "    def adjacency(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Connections c_{..} gathered in a matrix.\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray of shape (n,n)\n",
    "        \n",
    "        \"\"\"\n",
    "        adjacency = np.zeros((self.num_servers, self.num_servers), dtype=np.int8)\n",
    "        for (i, j) in self.connections:\n",
    "            adjacency[i, j] = 1\n",
    "            adjacency[j, i] = 1\n",
    "        return adjacency\n",
    "    \n",
    "RUNNING, CRASHED = 0, 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: The particular network Dieter is currently maintaining is fixed/defined below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = SysadminInstance(\n",
    "    num_servers=6,\n",
    "    connections=[(0,1), (1,2), (1,3), (2,3), (2,4), (3,5)],\n",
    "    alpha=0.5,\n",
    "    eps=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network dynamics\n",
    "Every day $t=0,1,2,...$, a server $i$ can be in one of two states\n",
    "\t$X_i^{(t)} \\in \\{0, 1\\}$, \n",
    "`RUNNING` ($0$), or `CRASHED` ($1$).\n",
    "Dieter's objective is to minimize\n",
    "\\begin{align*}\n",
    "\\sum_{t=0}^{\\infty} \\frac{\\gamma^t}{n} \\sum_{i=1}^n X_i^{(t)}, %& (\\gamma \\in [0,1])\n",
    "\\end{align*}\n",
    "the cumulative, discounted fraction of `CRASHED` servers.\n",
    "Right now ($t=0$), all servers are `RUNNING`.\n",
    "On each day $t$, Dieter must decide which of the servers should be maintained (only one can be maintained a time, $A_t \\in \\{1,\\ldots,n\\}$)\n",
    "such that its state on day $t+1$ is guaranteed to be `RUNNING`.\n",
    "All other `RUNNING` servers may randomly crash, depending on the state of their neighbors.\n",
    "Unmaintained servers that are already `CRASHED` remain `CRASHED`.\n",
    "More precisely, for all $i$\n",
    "\\begin{align*}\n",
    "P\\left(X_i^{(0)} = 0\\right) &= 1 \\\\\n",
    "P\\left(X_i^{(t+1)} = 0 \\mid A_t = i\\right) &= 1 \\\\\n",
    "P\\left(X_i^{(t+1)} = 1 \\mid A_t \\neq i, X^{(t)}_i = 0\\right) &= \\epsilon + \\frac{\\alpha C_i^{(t)}}{\\sum_{j} c_{ij}} \\\\\n",
    "P\\left(X_i^{(t+1)} = 1 \\mid A_t \\neq i, X^{(t)}_i = 1\\right) &= 1\n",
    "\\end{align*}\n",
    "where $C_i^{(t)} = \\sum_{j} c_{ij} X_i^{(t)}$ is the number of `CRASHED` neighbors of \n",
    "$i$ and $\\epsilon > 0$, $\\alpha > 0$, $\\epsilon + \\alpha \\leq 1$\n",
    "are additional parameters.\n",
    "\n",
    "Assume that *changes* in state of any two servers $i \\neq j$ occur independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task (I)\n",
    "*Briefly* describe the above maintenance task as an MDP. Your description does not need to be mathematically rigorous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer (I):**\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task (II)\n",
    "Complete the implementation of the environment class `SysadminEnv` below (that acts as a model-free simulator for the above MDP) by implementing a [step function and a reset function](https://gymnasium.farama.org/api/env/).\n",
    "- The `reset` function should reset the simulator and return the appropriate initial state.\n",
    "- The `step` function should advance the simulator by one step,  i.e.,\n",
    "  1. based on the current state and a given action, sample (from the correct distribution) a successor state\n",
    "  3. return the successor state, the appropriate reward & additional information as specified by the gymnasium docs.\n",
    "\n",
    "You may also extend other parts of the code.\n",
    "Your code will not be graded if it produces errors when run, or if it contains syntax errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SysadminEnv(gym.Env):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        instance: SysadminInstance,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.instance = instance\n",
    "        self.action_space = spaces.Discrete(self.instance.num_servers)\n",
    "        self.observation_space = spaces.MultiBinary(self.instance.num_servers)\n",
    "        ... # extend this if needed\n",
    "\n",
    "    def reset(\n",
    "        self, *, seed: int | None = None, options: dict[str, Any] | None = None\n",
    "    ) -> tuple[np.ndarray, dict[str, Any]]:\n",
    "        super().reset(seed=seed)\n",
    "        # TODO: implement this\n",
    "        initial_state = ...\n",
    "        ...\n",
    "        return initial_state\n",
    "   \n",
    "    def step(self, action: int) -> tuple[np.ndarray, float, bool, bool, dict[str, Any]]:\n",
    "        # TODO: implement this\n",
    "        next_state = ...\n",
    "        reward = ...\n",
    "        ...\n",
    "        return next_state, reward, False, False, dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.register(\"Sysadmin-EA2\", partial(SysadminEnv, instance))\n",
    "env = gym.make(\"Sysadmin-EA2\")\n",
    "state, _ = env.reset()\n",
    "state, reward, terminated, truncated, info = env.step(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task (III)\n",
    "Dieter's current maintenance policy $\\pi_D$ is given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomMaintenancePolicy:\n",
    "    def __init__(self, seed: int | None = None) -> None:\n",
    "        self.rng = np.random.default_rng(seed=seed)\n",
    "    \n",
    "    def __call__(self, state: np.ndarray) -> int:\n",
    "        # maintain uniformly at random if all servers running\n",
    "        if np.all(state == RUNNING):\n",
    "            return self.rng.integers(0, state.shape[0])\n",
    "        # randomly choose a crashed server for maintenance else\n",
    "        pi = self.rng.random(state.shape[0])\n",
    "        pi[state == RUNNING] = 0\n",
    "        return np.argmax(pi)\n",
    "\n",
    "policy_dieter = RandomMaintenancePolicy(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Help Dieter to estimate $\\mathbb{E}_{\\pi_D}[G_0]$\n",
    "Assume that the task is limited by a finite horizon, i.e. $t=0,\\dots,T-1$ with $\\gamma=1$ and $T=100$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proceed in the following way;\n",
    "1. Implement the function `sample_return` below.\n",
    "2. Call the function 100 times and compute the sample mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_return(\n",
    "    env: SysadminEnv,\n",
    "    policy: Callable[[np.ndarray], int],\n",
    "    horizon: int = 100,\n",
    "    gamma: float = 1,\n",
    "    seed: int | None = 42,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Perform a fixed amount of steps using actions from a provided policy\n",
    "    to generate a sample of the return \"G_0\" in a finite horizon/episodic setting\n",
    "    by accumulating discounted rewards for a fixed number of steps and returning the result.\n",
    "\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    env : SysadminEnv\n",
    "    policy : Callable[[np.ndarray], int]\n",
    "    horizon : int, (T)\n",
    "    gamma: float, discount factor\n",
    "    seed: int | None, environment random seed\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        A single sample of the return as described above. \n",
    "    \"\"\"\n",
    "    state, _ = env.reset(seed=seed)\n",
    "    # TODO\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "value_estimate = ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('rl23')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8aba6b08deb686889bfd57eea77364add1eb4e3ea064d848c1af21c0dc2449c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
